{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dev\\Miniconda3\\envs\\typst_ocr\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import onnx\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    VisionEncoderDecoderConfig,\n",
    "    TrOCRProcessor,\n",
    ")\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert `ORTModelForVision2Seq` ONNX model into `VisionEncoderDecoderModel` Torch model, we must find a map between ONNX model and Torch model. Therefore, we follow steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a `VisionEncoderDecoderModel` model with identical configuration and assign designed weights to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pix2text-mfr/config.json\", \"r\") as config_file:\n",
    "    config_data = json.load(config_file)\n",
    "\n",
    "config = VisionEncoderDecoderConfig.from_dict(config_data)\n",
    "torch_model = VisionEncoderDecoderModel(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, \"designed\" means we are confident to find it in a converted ONNX model. To satisfy that requirement, we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_weights(model):\n",
    "    counter1 = 0.01\n",
    "    counter2 = 0.02\n",
    "    for param in model.parameters():\n",
    "        new_values = torch.full(param.shape, counter1)\n",
    "\n",
    "        # Ensure there is a second element to modify\n",
    "        if param.numel() > 1:  \n",
    "            param_shape = param.shape\n",
    "            flattened_values = new_values.view(-1)\n",
    "            for i in range(1, len(flattened_values), param_shape[-1]):\n",
    "                # Modify the second element to ensure we can find it even after transpose\n",
    "                flattened_values[i] = counter2  \n",
    "        param.data = new_values\n",
    "        counter1 += 0.01\n",
    "        counter2 += 0.01\n",
    "\n",
    "modify_weights(torch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can simply save the model and convert it into ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\noptimum-cli export onnx --task image-to-text --model torch_model onnx_model\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model.save_pretrained(\"torch_model\")\n",
    "\n",
    "\"\"\"\n",
    "optimum-cli export onnx --task image-to-text --model torch_model onnx_model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finding the map between ONNX model weights and Torch model weights, so let's load ONNX model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model_path = \"onnx_model/encoder_model.onnx\"\n",
    "decoder_model_path = \"onnx_model/decoder_model.onnx\"\n",
    "\n",
    "encoder_onnx = onnx.load(encoder_model_path)\n",
    "decoder_onnx = onnx.load(decoder_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then extract everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weights_from_onnx(onnx_model):\n",
    "    weights = {}\n",
    "    for tensor in onnx_model.graph.initializer:\n",
    "        weights[tensor.name] = torch.tensor(onnx.numpy_helper.to_array(tensor))\n",
    "    return weights\n",
    "\n",
    "\n",
    "encoder_weights = extract_weights_from_onnx(encoder_onnx)\n",
    "decoder_weights = extract_weights_from_onnx(decoder_onnx)\n",
    "onnx_weights = {**encoder_weights, **decoder_weights}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need weights of Torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_weights = {name: param for name, param in torch_model.named_parameters()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match them.\n",
    "\n",
    "About why I transpose Torch weights and match them, it's a simple assumption: \n",
    "\n",
    "When exporting Torch model to ONNX model, optimum changed some steps of inference like $ AB = \\left(B^T A^T\\right)^T $, and therefore the name of those weights A or B are missing(You'll find something like `onnx::MatMul_1234`). \n",
    "\n",
    "So my solution is just transpose them back, and surprisingly it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: weight ['encoder.pooler.dense.weight', 'encoder.pooler.dense.bias'] mismatched!\n"
     ]
    }
   ],
   "source": [
    "def find_matching_weights(torch_weights, onnx_weights, tolerance=1e-3):\n",
    "    matches = []\n",
    "    mismatches = []\n",
    "    remaining_onnx_weights = onnx_weights.copy()\n",
    "\n",
    "    for torch_name, torch_weight in torch_weights.items():\n",
    "        match_found = False\n",
    "        for onnx_name, onnx_weight in remaining_onnx_weights.items():\n",
    "            if torch_weight.shape == onnx_weight.shape:\n",
    "                # Calculate the difference\n",
    "                difference = torch.abs(torch_weight - onnx_weight)\n",
    "                max_difference = torch.max(difference).item()\n",
    "                if max_difference < tolerance:\n",
    "                    matches.append((torch_name, onnx_name, False))\n",
    "                    match_found = True\n",
    "                    del remaining_onnx_weights[onnx_name]\n",
    "                    break\n",
    "\n",
    "        # If no match found, try with transposed weight\n",
    "        if not match_found:\n",
    "            transposed_weight = torch_weight.T\n",
    "            for onnx_name, onnx_weight in remaining_onnx_weights.items():\n",
    "                if transposed_weight.shape == onnx_weight.shape:\n",
    "                    # Calculate the difference\n",
    "                    difference = torch.abs(transposed_weight - onnx_weight)\n",
    "                    max_difference = torch.max(difference).item()\n",
    "                    if max_difference < tolerance:\n",
    "                        matches.append((torch_name, onnx_name, True))\n",
    "                        match_found = True\n",
    "                        del remaining_onnx_weights[onnx_name]\n",
    "                        break\n",
    "\n",
    "        if not match_found:\n",
    "            mismatches.append(torch_name)\n",
    "\n",
    "    return matches, mismatches\n",
    "\n",
    "matches, mismatches = find_matching_weights(torch_weights, onnx_weights)\n",
    "\n",
    "print(f\"Warning: weight {mismatches} mismatched!\")\n",
    "# Don't worry about warnings. It's okay missing those weights for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got the map, let's extract weights from ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model_path = \"pix2text-mfr/encoder_model.onnx\"\n",
    "decoder_model_path = \"pix2text-mfr/decoder_model.onnx\"\n",
    "\n",
    "encoder_onnx = onnx.load(encoder_model_path)\n",
    "decoder_onnx = onnx.load(decoder_model_path)\n",
    "\n",
    "encoder_weights = extract_weights_from_onnx(encoder_onnx)\n",
    "decoder_weights = extract_weights_from_onnx(decoder_onnx)\n",
    "onnx_weights = {**encoder_weights, **decoder_weights}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply load weights into a new torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights_to_torch(weight_map, onnx_weights, torch_model):\n",
    "    for torch_name, onnx_name, transpose in weight_map:\n",
    "        onnx_weight = onnx_weights[onnx_name]\n",
    "        if transpose:\n",
    "            onnx_weight = onnx_weight.T\n",
    "        torch_model_state_dict = torch_model.state_dict()\n",
    "        if torch_name in torch_model_state_dict:\n",
    "            torch_model_state_dict[torch_name].copy_(onnx_weight)\n",
    "        else:\n",
    "            print(f\"Warning: {torch_name} not found in the PyTorch model state dict.\")\n",
    "\n",
    "model = VisionEncoderDecoderModel(config=config)\n",
    "\n",
    "\n",
    "\n",
    "load_weights_to_torch(matches, onnx_weights, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test our Torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: testimg/1.png\n",
      "Decoded Output: ['x ^ { 2 } + 1 = 0 \\\\Rightarrow x = \\\\pm i']\n",
      "--------------------------------------------------\n",
      "Image: testimg/2.png\n",
      "Decoded Output: ['\\\\begin{array} { r l } { } & { { } \\\\operatorname* {']\n",
      "--------------------------------------------------\n",
      "Image: testimg/3.png\n",
      "Decoded Output: ['\\\\begin{aligned} { } & { { } \\\\operatorname* { l i m }']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(\"breezedeus/pix2text-mfr\")\n",
    "\n",
    "image_fps = [\n",
    "    \"testimg/1.png\",\n",
    "    \"testimg/2.png\",\n",
    "    \"testimg/3.png\",\n",
    "]\n",
    "\n",
    "for image_fp in image_fps:\n",
    "    images = [Image.open(image_fp).convert(\"RGB\")]\n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(pixel_values)\n",
    "\n",
    "    generated_text = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Image: {image_fp}\")\n",
    "    print(f\"Decoded Output: {generated_text}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save, of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"converted_torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, note that to ensure the model is standalone and works perfectly, you also need to copy some configurations like `generation_config.json` of original model to the `converted_torch` folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "typst_ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
